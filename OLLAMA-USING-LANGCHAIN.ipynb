{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e43940-ee0f-4828-86ec-9e52180e7a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.0.0 (from langchain-ollama)\n",
      "  Downloading langchain_core-1.2.9-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.6.0 in c:\\users\\bramh\\anaconda3\\envs\\ml\\lib\\site-packages (from langchain-ollama) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.3.45)\n",
      "Requirement already satisfied: packaging>=23.2.0 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.11.7)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.15.0)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.0.0->langchain-ollama)\n",
      "  Downloading uuid_utils-0.14.0-cp39-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\bramh\\anaconda3\\envs\\ml\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.11.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.32.5)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (2.0.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\bramh\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-ollama) (1.2.2)\n",
      "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Downloading langchain_core-1.2.9-py3-none-any.whl (496 kB)\n",
      "Downloading uuid_utils-0.14.0-cp39-abi3-win_amd64.whl (182 kB)\n",
      "Installing collected packages: uuid-utils, langchain-core, langchain-ollama\n",
      "\n",
      "  Attempting uninstall: langchain-core\n",
      "\n",
      "    Found existing installation: langchain-core 0.3.75\n",
      "\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "    Uninstalling langchain-core-0.3.75:\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "      Successfully uninstalled langchain-core-0.3.75\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ------------- -------------------------- 1/3 [langchain-core]\n",
      "   ---------------------------------------- 3/3 [langchain-ollama]\n",
      "\n",
      "Successfully installed langchain-core-1.2.9 langchain-ollama-1.0.1 uuid-utils-0.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain-cohere 0.3.5 requires langchain-core<0.4.0,>=0.3.27, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain-community 0.3.28 requires langchain-core<1.0.0,>=0.3.74, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain-experimental 0.3.4 requires langchain-core<0.4.0,>=0.3.28, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain-groq 0.3.7 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain-huggingface 0.3.1 requires langchain-core<1.0.0,>=0.3.70, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain-openai 0.3.32 requires langchain-core<1.0.0,>=0.3.74, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain-pinecone 0.2.12 requires langchain-core<1.0.0,>=0.3.34, but you have langchain-core 1.2.9 which is incompatible.\n",
      "langchain-text-splitters 0.3.9 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.2.9 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install  langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ced2ece-318c-4b2e-83a0-57f35112c8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum entanglement is a bizarre phenomenon where two or more particles become linked in such a way that they share the same fate, no matter how far apart they are â€“ measuring the state of one instantly influences the state of the other.\n"
     ]
    }
   ],
   "source": [
    "#FOR CHAT CONVERSATION\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:4b\",\n",
    "    temperature=0,\n",
    "    \n",
    ")\n",
    "# Simple invocation\n",
    "response = llm.invoke(\"Explain the concept of quantum entanglement in one sentence.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fbb9c5d-e40f-4a32-9d45-65fb44424597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. \n",
      "\n",
      "Do you want to know anything more about Paris or France?\n"
     ]
    }
   ],
   "source": [
    "# FOR NON CHAT EXECUTION\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"gemma3:4b\")\n",
    "\n",
    "response = llm.invoke(\"The capital of France is\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59115ae6-73c6-4e3f-8156-a56e3a935981",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"embeddinggemma:latest\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OllamaEmbeddings(\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddinggemma:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Embed a single query\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m query_result \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is LangChain?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(query_result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\langchain_ollama\\embeddings.py:307\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    306\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed query text.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\langchain_ollama\\embeddings.py:301\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    296\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama client is not initialized. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure Ollama is running and the model is loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m     )\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\ollama\\_client.py:393\u001b[0m, in \u001b[0;36mClient.embed\u001b[1;34m(self, model, input, truncate, options, keep_alive, dimensions)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\n\u001b[0;32m    385\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    386\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m   dimensions: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    392\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EmbedResponse:\n\u001b[1;32m--> 393\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEmbedResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/embed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEmbedRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\ollama\\_client.py:189\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    187\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\ollama\\_client.py:133\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 133\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[0;32m    135\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResponseError\u001b[0m: model \"embeddinggemma:latest\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"embeddinggemma:latest\",\n",
    ")\n",
    "\n",
    "# Embed a single query\n",
    "query_result = embeddings.embed_query(\"What is LangChain?\")\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812ee92-f5e4-4ca4-86da-af751becf5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
